# CMU 11-711: Advanced Natural Language Processing (ANLP)笔记（combined with 2024spring 2024fall 2025spring）


[toc]

## 0.before you read
当下以大语言模型llm为主导的的nlp领域发展迅速，日新月异，笔者从课程安排来看，几乎每年都有变化，内容也会有所变动。所以这份笔记也许也应当具有时效性并随着时间推移进行迭代。但无论如何，姑且记录，作为个人学习梳理

## 1.Introduction to NLP

### 自然语言处理问题总的的来说可以归结为如下几个问题

1.处理分析语言（给一段话，理解这段话的内容，指出其情感等，文本分类）

2.人与机器交互（llm回答问题，生成代码）

3.协助人人交互（语法检查，翻译）

3.5.以及cvnlp结合 img和文本之间的转换


### 我们现在做nlp是要做什么

为什么有些模型在有些方面表现得好（追求sota：state of the art ）

为什么现在的sota模型在一些方面仍然有问题？

我们该如何改进我们的模型？

### 构建nlp系统的方法
1.规则式构建模型（base on rules：如池袋，用一个固定的rule进行规范分类）不用经过训练

2.对没有训练的模型进行提示（大白话就是提示词工程！通过设计提示词，让llm在不再次训练的情况下达到我们的要求）

3.训练微调（微调：fine-tuning）
不只是llm，一些其他的基于训练的语言模型都是如此

### 动手构建最简单的规则式语言模型

如：一个判定情感的model

五步走：

1.利用一个函数，提取特征（features）如：设定看到正向词语加一分，反之扣一分

2.算得分

3.构建决策函数，根据得分算出对应的结果

4.准确性分析：根据计算出来的结果和实际数据结果对比，测评准确性

5.根据准确性测评结果，进行误差分析并进一步修改

当我们重复这个循环之后，我们的模型在训练数据上面（train set）已经表现得准确度很高了，那么我们把它拿到测试数据上（test set）测评,再根据结果进行更改变化。是经典的 训练 测试 验证 逻辑。

然而规则式语言模型会有如下问题：

1.低频词难以处理

2.合成词难以处理（意思相近形态不同）

3.否定词对句子产生的影响难以处理

4.隐喻类比（整体句意无法处理）

5.其他语言无法处理

因而引出：nlp based on Machine Learning

### ML for NLP

#### 第一个尝试：词袋模型（basgs of words,BOW）

每个词对应一个独热向量（one-hot vector），把句子中所有词向量加起来就是代表句子特征的向量，
乘以权重W

$Wx=scores$  

达到分类效果。通过ML，改进W矩阵

此处算法原理很朴素，每次训练结果对了词语的权重就加分反之减分，最后多轮训练可以通过最终输出的分数判断结果

#### BOW的缺陷

1.one-hot向量编码的问题：无法处理近义词和词语变形，词语数量一旦大了，词向量的长度会非常大，非常低效且浪费内存空间  

2.无法处理but，否定等句意相关信息，对于词语位置没有感知力

#### 改进：基于神经网络的模型

1.通过*复杂方法* 把word编码成词向量

2.通过*方法* 把词向量提取为句子特征

3.根据神经网络处理句子特征

之后我们的研究其实都基于此，无论是transformer还是别的，本质上是对步骤一和二中*方法* 的改进，我们要找到一个可以提取语言特征的工具

## 2.Word Representation and Text Classifiers

### Subword Models（子词模型）

#### SM原理
为了改进one-hot向量，采用“字节对编码”（Byte Pair Encoding, BPE）  
基本思路是统计句子中的字母组合出现次数 如：es er 然后将持续最多的组合记为一个子词，然后将子词作为整体，再持续循环，最终得到可以用来拆分词语的子词表，内容比如 ：er est pro ed  
于是可以处理词语变形，同时节省内存（可以用少量子词表示大量词语）  

在获得子词表之后，我们以一元语言模型（Unigram LM，后面涉及）为例，来说明如何进行子词分割  
通过一元语言模型，我们进行一些算法（这不太重要），最终通过ml获得一个 最优词汇表 可以通过它以及一些计算得到每个子词出现的概率  
然后我们检查目标句子，进行不同的拆分方式，比如：est 拆成 e st 还是es t 最后选用概率最大的拆分方式就是结果

#### 注意事项 
多语言方面，容易过度分割混合语言语料中的小语种  
解决思路：对小语种进行采样
在如 es t和e st分割抉择上容易出问题  
解决思路：通过“子词正则化”，在训练时对不同的分割结果进行采样以减少鲁棒性

### Continuous Word Embeddings（连续词嵌入）

对one—hot vector（独热向量）的大改进，使得用来表示各种词的向量长度大大减小，内存占用减少，同时具备了一些良好的性质，比如：“mom”-“female”和“dad”-“male”的词向量相似，近义词的词向量相似等  
我们会得到一个词向量库，每次解码（将词转化为向量）只需要查找（look up）到相应向量即可

### 如何训练更加复杂的模型（ML基础回顾）（不再赘述）

### Basic Idea of Neural Networks(for NLP Prediction Tasks) ：神经网络到底在干什么？

关键在于理解：提取并组合特征 

graph TD
    subgraph "输入层 (Input Layer)"
        A[I] --> B(lookup)
        C[hate] --> D(lookup)
        E[this] --> F(lookup)
        G[movie] --> H(lookup)
    end

    subgraph "词向量 (Word Embeddings)"
        B --> I[词向量];
        D --> I;
        F --> I;
        H --> I;
    end

    I --> J["复杂的函数提取组合特征<br>(神经网络)"];

    subgraph "输出层 (Output Layer)"
        J --> K[scores];
        K --> L(softmax);
        L --> M[probs];
    end








