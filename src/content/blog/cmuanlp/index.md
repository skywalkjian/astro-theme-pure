---
title: Notes of CMU ANLP 
publishDate: 2025-07 
description: 'cmu高级自然语言处理'
tags:
  - ai
  - nlp  
heroImage: { src: './thumbnail.jpg', color: '#B4C6DA' }
language: '中文&ENG'
---

# CMU 11-711: Advanced Natural Language Processing (ANLP)笔记（combined with 2024spring 2024fall 2025spring）




## 0.before you read
当下以大语言模型llm为主导的的nlp领域发展迅速，日新月异，笔者从课程安排来看，几乎每年都有变化，内容也会有所变动。所以这份笔记也许也应当具有时效性并随着时间推移进行迭代。但无论如何，姑且记录，作为个人学习梳理

## 1.Introduction to NLP

### 自然语言处理问题总的的来说可以归结为如下几个问题

1.处理分析语言（给一段话，理解这段话的内容，指出其情感等，文本分类）

2.人与机器交互（llm回答问题，生成代码）

3.协助人人交互（语法检查，翻译）

3.5.以及cvnlp结合 img和文本之间的转换


### 我们现在做nlp是要做什么

为什么有些模型在有些方面表现得好（追求sota：state of the art ）

为什么现在的sota模型在一些方面仍然有问题？

我们该如何改进我们的模型？

### 构建nlp系统的方法
1.规则式构建模型（base on rules：如池袋，用一个固定的rule进行规范分类）不用经过训练

2.对没有训练的模型进行提示（大白话就是提示词工程！通过设计提示词，让llm在不再次训练的情况下达到我们的要求）

3.训练微调（微调：fine-tuning）
不只是llm，一些其他的基于训练的语言模型都是如此

### 动手构建最简单的规则式语言模型

如：一个判定情感的model

五步走：

1.利用一个函数，提取特征（features）如：设定看到正向词语加一分，反之扣一分

2.算得分

3.构建决策函数，根据得分算出对应的结果

4.准确性分析：根据计算出来的结果和实际数据结果对比，测评准确性

5.根据准确性测评结果，进行误差分析并进一步修改

当我们重复这个循环之后，我们的模型在训练数据上面（train set）已经表现得准确度很高了，那么我们把它拿到测试数据上（test set）测评,再根据结果进行更改变化。是经典的 训练 测试 验证 逻辑。

然而规则式语言模型会有如下问题：

1.低频词难以处理

2.合成词难以处理（意思相近形态不同）

3.否定词对句子产生的影响难以处理

4.隐喻类比（整体句意无法处理）

5.其他语言无法处理

因而引出：nlp based on Machine Learning

### ML for NLP

#### 第一个尝试：词袋模型（basgs of words,BOW）

每个词对应一个独热向量（one-hot vector），把句子中所有词向量加起来就是代表句子特征的向量，
乘以权重W

$Wx=scores$


达到分类效果。通过ML，改进W矩阵

此处算法原理很朴素，每次训练结果对了词语的权重就加分反之减分，最后多轮训练可以通过最终输出的分数判断结果

#### BOW的缺陷

1.one-hot向量编码的问题：无法处理近义词和词语变形，词语数量一旦大了，词向量的长度会非常大，非常低效且浪费内存空间  

2.无法处理but，否定等句意相关信息，对于词语位置没有感知力

#### 改进：基于神经网络的模型

1.通过*复杂方法* 把word编码成词向量

2.通过*方法* 把词向量提取为句子特征

3.根据神经网络处理句子特征

之后我们的研究其实都基于此，无论是transformer还是别的，本质上是对步骤一和二中*方法* 的改进，我们要找到一个可以提取语言特征的工具

## 2.Word Representation and Text Classifiers

### Subword Models（子词模型）

#### SM原理
为了改进one-hot向量，采用“字节对编码”（Byte Pair Encoding, BPE）  
基本思路是统计句子中的字母组合出现次数 如：es er 然后将持续最多的组合记为一个子词，然后将子词作为整体，再持续循环，最终得到可以用来拆分词语的子词表，内容比如 ：er est pro ed  
于是可以处理词语变形，同时节省内存（可以用少量子词表示大量词语）  

在获得子词表之后，我们以一元语言模型（Unigram LM，后面涉及）为例，来说明如何进行子词分割  
通过一元语言模型，我们进行一些算法（这不太重要），最终通过ml获得一个 最优词汇表 可以通过它以及一些计算得到每个子词出现的概率  
然后我们检查目标句子，进行不同的拆分方式，比如：est 拆成 e st 还是es t 最后选用概率最大的拆分方式就是结果

#### 注意事项 
多语言方面，容易过度分割混合语言语料中的小语种  
解决思路：对小语种进行采样
在如 es t和e st分割抉择上容易出问题  
解决思路：通过“子词正则化”，在训练时对不同的分割结果进行采样以减少鲁棒性

### Continuous Word Embeddings（连续词嵌入）

对one—hot vector（独热向量）的大改进，使得用来表示各种词的向量长度大大减小，内存占用减少，同时具备了一些良好的性质，比如：“mom”-“female”和“dad”-“male”的词向量相似，近义词的词向量相似等  
我们会得到一个词向量库，每次解码（将词转化为向量）只需要查找（look up）到相应向量即可

### 如何训练更加复杂的模型（ML基础回顾）
不再赘述

### Basic Idea of Neural Networks(for NLP Prediction Tasks) 
神经网络到底在干什么？

关键在于理解：提取并组合“特征” 

![Local Image](src/assets/images/1.png)

所有我们的基于深度学习的任务都可以归纳为利用各种神经网络架构去提取句子的特征（呈现为一个向量）（图中左侧部分）。这个向量包含了语言的features，也就是这句话的所有的信息，之后我们再通过神经网络提取出为了完成我们目标任务所需要的信息，并根据此即可完成任务（体现为得到scores，然后依据他来得到一个结果）.

每一层的神经网络可以视为提取一个层次的特征，从低阶到高阶，第一层可能只是局部特征，比如词组结构，之后层数增加就可以在原有基础上提取更加抽象的特征，比如句子结构等关系。

特征有高阶有低阶，例如，每一个连续词向量的每一个维度都代表着一种特征，多轮学习之后的隐藏层向量的每个维度可能蕴含着特征，但是这种所谓特征并不是人为规定，而是经过ml之后自动学习出来的

## 3.Language and Sequence Modeling

### language models
分为生成式语言模型与判别式语言模型，本质都是概率语言模型  
$x~P(X)$ (X是词语或句子)  
生成式：预测下一个词  
判别式：预测label的概率进行分类  

### Auto-regressive Language Models（自回归模型）

$$
P(X) = \prod_{i=1}^{I} P(x_i \mid x_1, \dots, x_{i-1})
$$

$x_i$ :next token $x_i \mid x_1, \dots, x_{i-1}$:context

那么我们的关键在于，如何找到计算$P(x)$的方式

#### Unigram Language Models（单元语言模型）

core：最简单的语言模型，基本思想是，每一个词都与其他词独立，不会互相干扰  
$P(x_i | x_1, \dots, x_{i-1}) \approx P(x_i)$
著名例子：信息检索tf-idf算法  


#### 小tricks：log参数化
$$
P(X) = \prod_{i=1}^{I} P(x_i \mid x_1, \dots, x_{i-1})
$$
序列一大就会导致P(x)极其小，超出计算机能力范围。  
于是通过取对数，把乘积问题转化为加法问题，之后处理梯度爆炸和梯度消失的时候也会运用对数参数化方法  
$P(X) = \prod_{i=1}^{|X|} P(x_i) \longrightarrow \log P(X) = \sum_{i=1}^{|X|} \log P(x_i)$  

#### n-gram Models（n元语言模型）

在Unigram Language Models基础上加强了，认为 一个词出现的概率只与它前面固定数量的几个词有关
> ## llm解释多元与单元语言模型的区别（参考资料）
> 单元语言模型和多元语言模型是两种不同类型的语言模型，它们在结构和工作方式上有显著区别。
> 
> ### 单元语言模型 (Unigram Language Model)
>
> 单元语言模型是一种最简单的语言模型。它假设句子中的每个词都是独立出现的，与其他词没有关联。换句话说，一个词出现的概率只取决于它自身在整个语料库中出现的频率。
>
> **特点:**
  **独立性假设:** 忽略词与词之间的顺序和上下文关系。
  **计算简单:** 计算一个句子的概率时，只需将每个词的概率相乘即可。
  **局限性:** 无法捕捉语言中的语义和语法结构，因为它不知道词语之间的联系。
>
> **举例:**
> 假设我们有一个句子：“**我 喜欢 吃 苹果**”。
在一个单元语言模型中，它的概率计算方式是：
$P(\text{我 喜欢 吃 苹果}) = P(\text{我}) \times P(\text{喜欢}) \times P(\text{吃}) \times P(\text{苹果})$
>
> 其中，$P(\text{我})$、$P(\text{喜欢})$ 等概率都是根据每个词在训练语料库中出现的频率来计算的。这种模型无法区分“我喜欢吃苹果”和“苹果喜欢吃我”这样的句子，因为它只关心每个词出现的概率，而不关心词语的排列顺序。
>
> ---
>
> ### 多元语言模型 (N-gram Language Model)
>
> 多元语言模型是单元语言模型的扩展。它假设一个词出现的概率只与它前面固定数量的几个词有关，这个数量就是“n”。常见的有多元语法模型（N-gram），其中n=2时称为二元语法（Bigram），n=3时称为三元语法（Trigram），以此类推。
>
> **特点:**
> * **上下文依赖:** 考虑词语的顺序和上下文，能捕捉更丰富的语义信息。
> * **计算复杂:** 随着n的增加，需要计算和存储的概率数量会呈指数级增长。
> * **更接近自然语言:** 相比单元模型，它能更好地反映人类语言的语法和句法结构。
>
> **举例:**
依然是句子：“**我 喜欢 吃 苹果**”。
在一个**二元语法（2-gram）**模型中，它的概率计算方式是：
$P(\text{我 喜欢 吃 苹果}) = P(\text{我}) \times P(\text{喜欢}|\text{我}) \times P(\text{吃}|\text{喜欢}) \times P(\text{苹果}|\text{吃})$
>
> 这里，$P(\text{喜欢}|\text{我})$ 表示在“我”这个词出现之后，“喜欢”这个词出现的条件概率。这个模型考虑了前一个词对当前词的影响，所以它能更好地理解“我喜欢吃苹果”这个句子，因为它知道“我”后面出现“喜欢”的概率要比出现“苹果”的概率高得多。
>
> 随着n的增加，模型的表现会越来越好，但也面临“数据稀疏”问题，即很多n-gram组合在训练数据中可能从未出现过，导致概率为零。现代的大型语言模型（LLM）则超越了传统的N-gram模型，使用了更复杂的神经网络架构（如Transformer），能够处理更长的上下文和更复杂的语言依赖关系。

#### 多元语言模型的问题
1.近义词还是处理不了    
2.间隔词处理不好 (intervening words)：指的是在两个有强关联的词之间，存在其他不相关的词语。比如Mr.Perter Smith and Mr. Jane Smith,中间的词没有关系，但是产生较大影响  
3.n元规模不可能无限扩张，一般到7差不多了，所以对于长上下文依赖很难处理

#### 优势
那为什么我们还会用到他呢？  
因为他相比神经网络类语言模型更加高效快速，在处理一些比较简单的语言任务中发挥更好的性能，对计算资源要求低。所以我们经常会用他来处理原始的上游数据，经过n元语言模型处理后的数据在用神经网络语言模型

#### 基于神经网络的模型

初期：特征化模型，为后来的高级模型提供铺垫。例子：前馈神经网络语言模型

![Local Image](src/assets/images/2.png)

将词向量拼接成一个长向量，然后通过tanh激活函数和W1将其转化为低阶的隐藏层，在这个过程中W1的每一行都与行向量进行点积并加以偏置，获得了两个词向量的融合特征（原本的词向量，每个维度都代表着这个词的某一种特征），得到的新向量每一行都是新的融合特征，随后再来一次获得scores。这样，相似的词语有着相似的词向量，隐藏层也相似。进而可以处理同义词。而通过向量拼接并一起进行机器学习，可以将连续的几个词同时作为输入，也可以解决干预词的问题。但是由于拼接无可能无限长，而且必须认为预先定好数量，所以还是无法解决长距离语义依赖的问题


